---
title: Accelerating PoC Development with ChatGPT and Red Hat OpenShift
date: '2023-01-30'
tags: ['Chat GPT', 'Open Source', 'Digital Health', 'Kafka', 'Edge Computing']
draft: false
summary: 'How to derive the OLS Estimator with matrix notation and a tour of math typesetting using markdown with the help of KaTeX.'
---

## Background

As a solutions architect, I spend most of my days doodling boxes and lines. Sure, I know how to code, but I'm not one of those 10x engineers you hear about. However, with the recent buzz around AI/ML models like GPT-3, I thought to myself, "Hey, why not give it a shot?"

My goal was simple: build a functional proof of concept app in a matter of hours using technologies I'm familiar with architecturally, but have limited hands-on experience with. And so, with digital health as my target, I set out on a wild and wacky journey. Could I do it with just my mad skills and a lot of help from conversational AI? Let's find out!

The source-code to my application is available [here](https://github.com/adendl/DigitalHealth-Edge).

## The Application

I wanted to build a relatively simple application that I could expand on later. I landed on a basic, and ultimately pretty useless application that would take in healthcare data from a medical device, stream it through to a Kafka cluster, store it into a database - and then have a frontend application visualise that healthcare data in real time. The application would consist of the following components:

![Image](https://github.com/adendl/adl-blog/blob/master/public/static/images/chatgpt-poc-development/architecture.png?raw=true)
| Application | Component |
| --------------------------- | ---------------------------- |
| Patient Vitals Data Generator | Node.js |
| Patient Vitals API - Kafka Producer | Node.js + Express |
| Kafka Cluster | Red Hat OpenShift Streams for Apache Kafka |
| Kafka Consumer | Python |
| Database | MongoDB |
| Data Access API | Node.js + Express |
| Data Visualisation Frontend | React + Highcharts |

Here is where Red Hat OpenShift comes in. Using [OpenShift Local](https://developers.redhat.com/products/openshift-local/overview), a lightweight distribution of OpenShift made to run on development machines - I could quickly containerise and deploy the code that I had written.

For certain application components, such as my Apache Kafka cluster, and my MongoDB database - I was able to quickly select and deploy these from within OpenShift itself.

### Deploying my Application

Let's talk about deploying your application with OpenShift and containers. OpenShift provides two deployment methods: Source-to-Image (S2I) and direct container deployment, which can make your life a lot easier.

S2I is a great option if you're new to containers. Just point OpenShift to your GitHub repo and let it handle the rest. It detects the runtime and creates a container image, which can be run in your OpenShift cluster.

On the other hand, if you want more control over the containerization process, you can create your own container images using a Containerfile and Podman. This option is perfect for developers who want to customize their container images to their specific needs. And if you need help with the Containerfile, ChatGPT is always here to assist you.

## Building the Application

### Test JSON Data

Firstly, I needed some mock healthcare data to feed into my application. I asked ChatGPT to

```
generate a JSON object which includes information
about a patient's key vitals in an ER scenario.
```

```json:patient_vitals_payload_example.json
{
    "patient_id": "123456",
    "timestamp": "2022-12-01T10:30:00Z",
    "heart_rate": 72,
    "blood_pressure": {
        "systolic": 120,
        "diastolic": 80
    },
    "ecg_reading": {
        "rr_interval": 800,
        "p_wave": 0.05,
        "qrs_complex": 0.1,
        "t_wave": 0.2
    },
    "oxygen_saturation": 96,
    "heart_rate_variability": {
        "mean_rr": 792,
        "sdnn": 24,
        "rmssd": 36
    },
    "blood_chemistry": {
        "ldl": 130,
        "hdl": 40,
        "triglycerides": 150,
        "total_cholesterol": 220
    }
}
```

Hmmm...probably not super accurate - but I don't know enough to say otherwise. It will do.

### Node.js - Data Generator Service

Next, in lieu of a medical device to generate this information, I wanted to create a basic node.js application that would create some small variations on this data, and then post it to a REST API endpoint on a regular basis - every 0.5 seconds.
Copy and pasting the above JSON object, I asked ChatGPT

```
Based on this JSON payload, create a node.js application that generates
slightly different randomised values for each of the attributes,
and then POSTs it to the /vitals endpoint at a regular frequency.
Use HOSTURL and TIMEOUTTIME as environment variables to set the
hosturl and the interval.
```

```js:data_generator.js
const request = require('request');

const hostUrl = process.env.HOSTURL
const timeoutTime = process.env.TIMEOUTTIME

    async function sendRequest(interval) {
        while (true)
        {
            const patientData = {
                "patient_id": "123456",
                "timestamp": new Date().toISOString(),
                "heart_rate": Math.floor(Math.random() * 10) + 70,
                /** Removed for brevity
                **/
            await sleep(interval)
            request.post({
                url: `${hostUrl}/vitals`,
                json: patientData
            }, (error, response, body) => {
                if (error) {
                    console.log(error);
                } else {
                    console.log(response.statusCode, body);
                }
            });
        }
    }

    function sleep(ms) {
        return new Promise((resolve) => {
          setTimeout(resolve, ms);
        });
      }

sendRequest(timeoutTime);
```

### Node.js Express - Vitals API

Next up, to create a Node.js express server which is listening in on the /vitals endpoint to receive the message, and act as a Kafka producer and send it to a local Kafka cluster.

```
Based on the previous JSON payload, create a node.js express application
which listens on the /vitals endpoint, and then send that message to
a Kafka cluster, the Kafka connection string is an environment variable
called HOSTURL.
```

```js:vitals_api.js
const express = require('express');
const { Kafka } = require('kafkajs');
const bodyParser = require('body-parser');

const hostUrl = process.env.HOSTURL;
const app = express();
app.use(bodyParser.json());

const kafka = new Kafka({
  clientId: 'my-app',
  brokers: [hostUrl],
});
const producer = kafka.producer();

app.post('/vitals', async (req, res) => {
  const vitals = req.body;
  try {
    await producer.connect();
    await producer.send({
        topic: 'vitals',
        messages: [{ value: JSON.stringify(vitals) }],
    });
    res.status(200).send({ message: vitals });
    console.log("message sent");
  } catch (err) {
    console.log(err);
    res.status(500).send({ message: err });
  }
});

app.listen(3000, () => {
  console.log('Microservice listening on port 3000!');
});
```

### Kafka

Now to stand up a Kafka cluster. To do this, I made use of OpenShift operators to stand up a Kafka cluster. Operators automate the creation, configuration, and management of instances of Kubernetes-native applications.

![Image](https://github.com/adendl/adl-blog/blob/master/public/static/images/chatgpt-poc-development/operator.png?raw=true)

After installing the operator, I deployed the Red Hat Kafka cluster to my namespace.
![Image](https://github.com/adendl/adl-blog/blob/master/public/static/images/chatgpt-poc-development/deployed-operator.png?raw=true)

I then grab the Kafka broker connection and add it as an environment variable to my applications so that they can connect to the Kafka cluster.

```
my-cluster-kafka-brokers:9092
```

To provide a WebUI for my Kafka cluster, I used the Open-source project [Kafdrop](https://github.com/obsidiandynamics/kafdrop). I deployed the [Kafdrop Docker image](https://hub.docker.com/r/obsidiandynamics/kafdrop) directly into the cluster, and used the KAFKA_BROKERCONNECT environment variable and used the same string as above.

Inline or manually enumerated footnotes are also supported. Click on the links above to see them in action.

[^2]: \$10 and &dollar;20.

# Deriving the OLS Estimator

Using matrix notation, let $n$ denote the number of observations and $k$ denote the number of regressors.

The vector of outcome variables $\mathbf{Y}$ is a $n \times 1$ matrix,

```tex
\mathbf{Y} = \left[\begin{array}
  {c}
  y_1 \\
  . \\
  . \\
  . \\
  y_n
\end{array}\right]
```

$$
\mathbf{Y} = \left[\begin{array}
  {c}
  y_1 \\
  . \\
  . \\
  . \\
  y_n
\end{array}\right]
$$

The matrix of regressors $\mathbf{X}$ is a $n \times k$ matrix (or each row is a $k \times 1$ vector),

```latex
\mathbf{X} = \left[\begin{array}
  {ccccc}
  x_{11} & . & . & . & x_{1k} \\
  . & . & . & . & .  \\
  . & . & . & . & .  \\
  . & . & . & . & .  \\
  x_{n1} & . & . & . & x_{nn}
\end{array}\right] =
\left[\begin{array}
  {c}
  \mathbf{x}'_1 \\
  . \\
  . \\
  . \\
  \mathbf{x}'_n
\end{array}\right]
```

$$
\mathbf{X} = \left[\begin{array}
  {ccccc}
  x_{11} & . & . & . & x_{1k} \\
  . & . & . & . & .  \\
  . & . & . & . & .  \\
  . & . & . & . & .  \\
  x_{n1} & . & . & . & x_{nn}
\end{array}\right] =
\left[\begin{array}
  {c}
  \mathbf{x}'_1 \\
  . \\
  . \\
  . \\
  \mathbf{x}'_n
\end{array}\right]
$$

The vector of error terms $\mathbf{U}$ is also a $n \times 1$ matrix.

At times it might be easier to use vector notation. For consistency, I will use the bold small x to denote a vector and capital letters to denote a matrix. Single observations are denoted by the subscript.

## Least Squares

**Start**:  
$$y_i = \mathbf{x}'_i \beta + u_i$$

**Assumptions**:

1. Linearity (given above)
2. $E(\mathbf{U}|\mathbf{X}) = 0$ (conditional independence)
3. rank($\mathbf{X}$) = $k$ (no multi-collinearity i.e. full rank)
4. $Var(\mathbf{U}|\mathbf{X}) = \sigma^2 I_n$ (Homoskedascity)

**Aim**:  
Find $\beta$ that minimises the sum of squared errors:

$$
Q = \sum_{i=1}^{n}{u_i^2} = \sum_{i=1}^{n}{(y_i - \mathbf{x}'_i\beta)^2} = (Y-X\beta)'(Y-X\beta)
$$

**Solution**:  
Hints: $Q$ is a $1 \times 1$ scalar, by symmetry $\frac{\partial b'Ab}{\partial b} = 2Ab$.

Take matrix derivative w.r.t $\beta$:

```tex
\begin{aligned}
  \min Q           & = \min_{\beta} \mathbf{Y}'\mathbf{Y} - 2\beta'\mathbf{X}'\mathbf{Y} +
  \beta'\mathbf{X}'\mathbf{X}\beta \\
                   & = \min_{\beta} - 2\beta'\mathbf{X}'\mathbf{Y} + \beta'\mathbf{X}'\mathbf{X}\beta \\
  \text{[FOC]}~~~0 & =  - 2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\hat{\beta}                  \\
  \hat{\beta}      & = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}                              \\
                   & = (\sum^{n} \mathbf{x}_i \mathbf{x}'_i)^{-1} \sum^{n} \mathbf{x}_i y_i
\end{aligned}
```

$$
\begin{aligned}
  \min Q           & = \min_{\beta} \mathbf{Y}'\mathbf{Y} - 2\beta'\mathbf{X}'\mathbf{Y} +
  \beta'\mathbf{X}'\mathbf{X}\beta \\
                   & = \min_{\beta} - 2\beta'\mathbf{X}'\mathbf{Y} + \beta'\mathbf{X}'\mathbf{X}\beta \\
  \text{[FOC]}~~~0 & =  - 2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\hat{\beta}                  \\
  \hat{\beta}      & = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}                              \\
                   & = (\sum^{n} \mathbf{x}_i \mathbf{x}'_i)^{-1} \sum^{n} \mathbf{x}_i y_i
\end{aligned}
$$

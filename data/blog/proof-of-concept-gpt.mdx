---
title: Accelerating PoC Development with ChatGPT and Red Hat OpenShift
date: '2023-01-30'
tags: ['Chat GPT', 'Open Source', 'Digital Health', 'Kafka', 'Edge Computing']
draft: false
summary: 'How to use ChatGPT with OpenShift Application Development Platform to build quick proof of concept applications'
---

## Background

As a solutions architect, I spend most of my days doodling boxes and lines. Sure, I know how to code, but I'm not one of those 10x engineers you hear about. However, with the recent buzz around AI/ML models like GPT-3, I thought to myself, "Hey, why not give it a shot?"

My goal was simple: build a functional proof of concept app in a matter of hours using technologies I'm familiar with architecturally, but have limited hands-on experience with. And so, I set out on a wild and wacky journey. Could I do it with just my mad skills and a lot of help from conversational AI? Let's find out!

The source-code to my application is available [here](https://github.com/adendl/DigitalHealth-Edge).

## The Application

I wanted to build a relatively simple application that I could expand on later. I landed on a basic, and ultimately pretty useless application that would take in healthcare data from a medical device, stream it through to a Kafka cluster, store it into a database - and then have a frontend application visualise that healthcare data in real time. The application would consist of the following components:

![Image](https://github.com/adendl/adl-blog/blob/master/public/static/images/chatgpt-poc-development/architecture.png?raw=true)
| Application | Component |
| --------------------------- | ---------------------------- |
| Patient Vitals Data Generator | Node.js |
| Patient Vitals API - Kafka Producer | Node.js + Express |
| Kafka Cluster | Red Hat OpenShift Streams for Apache Kafka |
| Kafka Web UI | Kafdrop |
| Kafka Consumer | Python |
| Database | MongoDB |
| Data Access API | Node.js + Express |
| Data Visualisation Frontend | React + Highcharts |

Using [Red Hat OpenShift Local](https://developers.redhat.com/products/openshift-local/overview), a lightweight distribution of OpenShift made to run on development machines - I could quickly build and deploy a cloud-native kubernetes application.

For certain application building blocks, such as my Apache Kafka cluster, and my MongoDB database - I was able to quickly deploy these from within OpenShift itself.

### Deploying my Application

OpenShift provides two main deployment methods: Source-to-Image (S2I) and direct container deployment.

S2I is a great option if you're new to containers. Just point OpenShift to your GitHub repo and let it handle the rest. It detects the runtime and creates a container image which can be deployed in your OpenShift cluster.

On the other hand, if you want more control over the containerisation process, you can create your own container images using a Dockerfile and the Docker CLI (or alternatively Podman). This option is perfect for developers who want to customise their container images to their specific needs. And if you need help with the Containerfile, ChatGPT is pretty excellent at this.

## Building the Application

### Test JSON Data

Firstly, I needed some mock healthcare data to feed into my application. I asked ChatGPT to

```
generate a JSON object which includes information
about a patient's key vitals in an ER scenario.
```

```json:patient_vitals_payload_example.json
{
    "patient_id": "123456",
    "timestamp": "2022-12-01T10:30:00Z",
    "heart_rate": 72,
    "blood_pressure": {
        "systolic": 120,
        "diastolic": 80
    },
    "ecg_reading": {
        "rr_interval": 800,
        "p_wave": 0.05,
        "qrs_complex": 0.1,
        "t_wave": 0.2
    },
    "oxygen_saturation": 96,
    "heart_rate_variability": {
        "mean_rr": 792,
        "sdnn": 24,
        "rmssd": 36
    },
    "blood_chemistry": {
        "ldl": 130,
        "hdl": 40,
        "triglycerides": 150,
        "total_cholesterol": 220
    }
}
```

Hmmm...probably not super accurate - but I don't know enough to say otherwise. It will do.

### Node.js - Data Generator Service

Next, in lieu of a medical device to generate this information, I wanted to create a basic node.js application that would create some small variations on this data, and then post it to a REST API endpoint on a regular basis.
Copy and pasting the above JSON object, I asked ChatGPT

```
Based on this JSON payload, create a node.js application that generates
slightly different randomised values for each of the attributes,
and then POSTs it to the /vitals endpoint at a regular frequency.
Use HOSTURL and TIMEOUTTIME as environment variables to set the
hosturl and the interval.
```

```js:data_generator.js
const request = require('request');

const hostUrl = process.env.HOSTURL
const timeoutTime = process.env.TIMEOUTTIME

    async function sendRequest(interval) {
        while (true)
        {
            const patientData = {
                "patient_id": "123456",
                "timestamp": new Date().toISOString(),
                "heart_rate": Math.floor(Math.random() * 10) + 70,
                /** Removed for brevity
                **/
            await sleep(interval)
            request.post({
                url: `${hostUrl}/vitals`,
                json: patientData
            }, (error, response, body) => {
                if (error) {
                    console.log(error);
                } else {
                    console.log(response.statusCode, body);
                }
            });
        }
    }

    function sleep(ms) {
        return new Promise((resolve) => {
          setTimeout(resolve, ms);
        });
      }

sendRequest(timeoutTime);
```

### Node.js Express - Vitals API

Next up, to create a Node.js express server which is listening in on the /vitals endpoint to receive the message, and act as a Kafka producer and send it to my cluster.

```
Based on the previous JSON payload, create a node.js express application
which listens on the /vitals endpoint, and then send that message to
a Kafka cluster, the Kafka connection string is an environment variable
called HOSTURL.
```

```js:vitals_api.js
const express = require('express');
const { Kafka } = require('kafkajs');
const bodyParser = require('body-parser');

const hostUrl = process.env.HOSTURL;
const app = express();
app.use(bodyParser.json());

const kafka = new Kafka({
  clientId: 'my-app',
  brokers: [hostUrl],
});
const producer = kafka.producer();

app.post('/vitals', async (req, res) => {
  const vitals = req.body;
  try {
    await producer.connect();
    await producer.send({
        topic: 'vitals',
        messages: [{ value: JSON.stringify(vitals) }],
    });
    res.status(200).send({ message: vitals });
    console.log("message sent");
  } catch (err) {
    console.log(err);
    res.status(500).send({ message: err });
  }
});

app.listen(3000, () => {
  console.log('Microservice listening on port 3000!');
});
```

### Kafka

To stand up my Kafka cluster, I made use of OpenShift operators to stand up a Kafka cluster. Operators automate the creation, configuration, and management of instances of Kubernetes-native applications.

![Image](https://github.com/adendl/adl-blog/blob/master/public/static/images/chatgpt-poc-development/operator.png?raw=true)

After installing the operator, I deployed the Red Hat Kafka cluster to my namespace.
![Image](https://github.com/adendl/adl-blog/blob/master/public/static/images/chatgpt-poc-development/deployed-operator.png?raw=true)

I then grab the Kafka broker host name and add it as an environment variable to my applications so that I can establish connectivity.

```
my-cluster-kafka-brokers:9092
```

To provide a WebUI for my Kafka cluster, I used the Open-source project [Kafdrop](https://github.com/obsidiandynamics/kafdrop). I deployed the [Kafdrop Docker image](https://hub.docker.com/r/obsidiandynamics/kafdrop) and used the `KAFKA_BROKERCONNECT` environment variable and the same string as above. Kafdrop is pretty neat, you can manage and visualise topics, partitions, and even set up RBAC controls for your Kafka resources.

![Image](https://github.com/adendl/adl-blog/blob/master/public/static/images/chatgpt-poc-development/kafdrop.png?raw=true)

### Python - Kafka Consumer

I can verify that messages are flowing through into Kafka, now to create a Kafka consumer that can poll these messages. For this first build of the application, I just want the Kafka consumer to store these messages in a MongoDB instance.

```
Based on the Kafka message above, create a Python application which
consumes that message, and writes it to a local MongoDB instance.
Use two environment variables, HOSTURL and DATABASE_URL to
pass the Kafka broker string and the mongodb connection string.
```

```py:main.py
//import dependencies
//set ENV variables
# Connect to Kafka consumer
consumer = KafkaConsumer('vitals', bootstrap_servers=HOSTURL)

# Connect to MongoDB
try:
    client = MongoClient(DATABASE_URL)
    db = client.vitals
    vitals_collection = db.get_collection('vitals')
except Exception as e:
    print(f"Error connecting to MongoDB: {e}")
    exit()

for msg in consumer:
    try:
        msg_value = json.loads(msg.value)
        vitals_collection.insert_one(msg_value)
        print("Inserted into MongoDB: ", msg_value)
    except Exception as e:
        print(f"Error inserting document into MongoDB: {e}")
```

### MongoDB

To create a local MongoDB instance, I select a default MongoDB image from the OpenShift developer catalogue and deploy it next to my application.
![Image](https://github.com/adendl/adl-blog/blob/master/public/static/images/chatgpt-poc-development/mongo.gif?raw=true)

### Node.js Express - Data Access API

Nearly there....to expose the data stored in my MongoDB database to my frontend application, I created a simple data access API.

```
Create a node.js express data access API, that connects to the MongoDB
cluster, queries the vitals collection for the 100 most recent entries,
and returns this data in JSON format.
```

```js:index.js
/** Install Dependencies **/
app.use(cors())

// Connection URL
const url = process.env.DATABASE_URL;
// Database Name
const dbName = 'vitals';
// Create a new MongoClient
const client = new MongoClient(url);

// Use connect method to connect to the server
client.connect(function(err) {
  console.log("Connected to MongoDB server");
  const db = client.db(dbName);

  // Routes
  app.get("/data", (req, res) => {
    // Find all documents in the "vitals" collection
    db.collection("vitals").find({}).limit(100).toArray(function(err, result) {
      if (err) throw err;
      res.json(result);
    });
  });
});
// Start the server
app.listen(3005, () => {
  console.log("Server started on port 3005");
});
```

### React + Highcharts - Frontend

To visualise this data, I wanted to create a basic frontend web-app that would query the data access API on a regular interval, and then visualise this data through some basic graphs.

For this, after a few minutes of Googling, I chose the Highcharts library for React. As with every time I do anything frontend, I ran into CORS issues that I spent half an hour troubleshooting.

```
Create a Highcharts React application that queries the data
access API I previously created, and then visualises the
heart rate in a basic time series chart
```

```js:App.js
/**Import Dependencies **/
class ChartComponent extends Component {
  state = {
    data: []
  };
  componentDidMount() {
    this.fetchData();
    setInterval(() => {
      this.fetchData();
    }, 1000);
  }
  fetchData() {
    fetch("http://data-access-api-digitalhealth-edge.apps-crc.testing/data")
      .then(response => response.json())
      .then(data => {
        this.setState({ data });
      });
  }
  render() {
    /** render options **/
    };
    return (
      <div>
        <HighchartsReact highcharts={Highcharts} options={options} />
      </div>
    );
  }
}
export default ChartComponent;
```

Hey, it works!
![Image](https://github.com/adendl/adl-blog/blob/master/public/static/images/chatgpt-poc-development/frontend.png?raw=true)

### The Final Product

Now that all of the different components have been built, I deployed all of these as containers into OpenShift.

![Image](https://github.com/adendl/adl-blog/blob/master/public/static/images/chatgpt-poc-development/application.png?raw=true)

## Learnings

In just a few hours, I was able to create a basic proof of concept. Prior to this, I would have spent a significant amount of time researching various technologies and their implementations. The thing is, if I don't have experience in application development, I still will have been pretty lost. I think this meme illustrates it pretty well:
![Image](https://github.com/adendl/adl-blog/blob/master/public/static/images/chatgpt-poc-development/meme.png?raw=true)

Utilising OpenShift made a remarkable difference. Troubleshooting container logs and incorporating application building blocks such as Kafka and MongoDB was easy, leading to a straightforward deployment of the application. If I were to move this application to a production setting, I would've also found that the inbuilt logging and telemetry provided by OpenShift makes running the application itself a lot easier as well.

## What's next?

I plan on elevating the project by introducing additional components, such as the Apache CamelK serverless integration framework, as well as machine learning models and ML Ops - With the help of Red Hat Openshift Data Science.
